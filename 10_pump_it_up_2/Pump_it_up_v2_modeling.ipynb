{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pump it up - Data mining the water table\n",
    "#### https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/\n",
    "\n",
    "### Balazs Balogh - 01. 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the packages.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # explicitly require this experimental feature\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier # now you can import normally from ensemble\n",
    "\n",
    "from sklearn import tree, ensemble, metrics, svm\n",
    "from xgboost import *\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_selected = pd.read_csv('https://raw.githubusercontent.com/budapestpy-workshops/workshops/master/10_pump_it_up_2/pumpitup_train_preporcessed.csv')\n",
    "test_selected = pd.read_csv('https://raw.githubusercontent.com/budapestpy-workshops/workshops/master/10_pump_it_up_2/pumpitup_test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_selected['label']\n",
    "features = data_selected.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train original shape: (47520, 69)\n",
      "X_train shape after creating validation set: (40392, 69)\n",
      "X_val shape: (7128, 69)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creating the train / test / validation datasets.\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, train_size=0.8, stratify=target, random_state=123)\n",
    "print('X_train original shape:', X_train.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.85, stratify=y_train, random_state=123)\n",
    "print('X_train shape after creating validation set:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [(0, 15520), (1, 2936), (2, 21936)]\n",
      "Resampled data: [(0, 21936), (1, 21936), (2, 21936)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Oversampling example with SMOTE\n",
    "\n",
    "https://towardsdatascience.com/sampling-techniques-for-extremely-imbalanced-data-part-ii-over-sampling-d61b43bc4879\n",
    "https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "X_resampled, y_resampled = SMOTE().fit_resample(X_train, y_train)\n",
    "\n",
    "print('Original data:', sorted(Counter(y_train).items()))\n",
    "print('Resampled data:', sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "### Experimenting with PCA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Dimension reduction with PCA.\n",
    "\n",
    "First I tried to reduce from 121 to 30 components, it scored 72% on the validation set.\n",
    "\"\"\"\n",
    "data_selected_scaled = StandardScaler().fit_transform(data_selected.drop('label', axis=1))\n",
    "data_selected_scaled = pd.DataFrame(data_selected_scaled)\n",
    "\n",
    "pca = PCA(n_components=30)\n",
    "x_pca = pca.fit_transform(data_selected_scaled)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "x_pca.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "StandardScaled edition.\n",
    "Using x_pca for train values.\n",
    "\n",
    "Creating the train / test / validation datasets.\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_pca, target, train_size=0.8, stratify=target)\n",
    "print('X_train original shape:', X_train.shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.85, stratify=y_train)\n",
    "print('X_train shape after creating validation set:', X_train.shape)\n",
    "print('X_val shape:', X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.63      0.71      4565\n",
      "           1       0.59      0.10      0.17       863\n",
      "           2       0.72      0.91      0.81      6452\n",
      "\n",
      "    accuracy                           0.75     11880\n",
      "   macro avg       0.71      0.55      0.56     11880\n",
      "weighted avg       0.75      0.75      0.72     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "xgboost_classifier = XGBClassifier(eta=0.075, max_depth=3, min_child_weight=0, n_estimators=200, scale_pos_weigth=5,\n",
    "                                   objective='multi:softmax', eval_metric='merror', num_class=3)\n",
    "\n",
    "xgboost = xgboost_classifier.fit(X_train, y_train.values.ravel())\n",
    "# xgboost = xgboost_classifier.fit(X_resampled, y_resampled.ravel()) # With SMOTE oversampling data\n",
    "\n",
    "expected = y_test\n",
    "predicted = xgboost.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.62      0.71      2739\n",
      "           1       0.60      0.11      0.18       518\n",
      "           2       0.72      0.92      0.80      3871\n",
      "\n",
      "    accuracy                           0.74      7128\n",
      "   macro avg       0.71      0.55      0.56      7128\n",
      "weighted avg       0.75      0.74      0.72      7128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the validation set, which is unseen for the model.\n",
    "\"\"\"\n",
    "\n",
    "validation_expected = y_val\n",
    "validation_predicted = xgboost.predict(X_val)\n",
    "\n",
    "print(metrics.classification_report(validation_expected, validation_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>waterpoint_type_group_other</td>\n",
       "      <td>263.982527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quantity_insufficient</td>\n",
       "      <td>159.144926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>extraction_type_class_other</td>\n",
       "      <td>144.052858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quantity_enough</td>\n",
       "      <td>108.298299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>quantity_seasonal</td>\n",
       "      <td>84.241811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>construction_year_70s</td>\n",
       "      <td>68.174820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>funder_gov</td>\n",
       "      <td>66.466003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amount_tsh</td>\n",
       "      <td>48.297427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>construction_year_80s</td>\n",
       "      <td>45.533163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>extraction_type_class_motorpump</td>\n",
       "      <td>41.511333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature_name  importance\n",
       "0       waterpoint_type_group_other  263.982527\n",
       "2             quantity_insufficient  159.144926\n",
       "13      extraction_type_class_other  144.052858\n",
       "1                   quantity_enough  108.298299\n",
       "23                quantity_seasonal   84.241811\n",
       "16            construction_year_70s   68.174820\n",
       "18                       funder_gov   66.466003\n",
       "3                        amount_tsh   48.297427\n",
       "32            construction_year_80s   45.533163\n",
       "28  extraction_type_class_motorpump   41.511333"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get feature importances.\n",
    "Of course, this works without PCA, so when we know the original columns.\n",
    "\n",
    "\"The Gain is the most relevant attribute to interpret the relative importance of each feature.\"\n",
    "\"\"\"\n",
    "\n",
    "xgb_fi = xgboost.get_booster().get_score(importance_type=\"gain\")\n",
    "\n",
    "xgboost_feature_importances = pd.DataFrame(list(xgb_fi.items()), columns=['feature_name', 'importance'])\n",
    "\n",
    "xgboost_feature_importances.sort_values(by=['importance'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GridSearchCV, the most computation expensive part of the process. With a relatively small amount of parameters, it lifted\n",
    "the performance from 73.9% to 75.2%.\n",
    "\"\"\"\n",
    "\n",
    "xgboost = XGBClassifier(objective='multi:softmax', eval_metric='merror', num_class=3, n_jobs=-1)\n",
    "\n",
    "params = {'n_estimators': [100, 200, 300],\n",
    "          'max_depth': [5, 8, 15],\n",
    "          'learning_rate': [0.01, 0.075, 0.1, 0.2],\n",
    "          'min_child_weight': [3, 5, 7]\n",
    "         }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgboost, cv=4, param_grid=params, n_jobs=-1, verbose=10)\n",
    "\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross validation took about 5 minutes, and it showed a 74% accuracy. We can see each fold, the model is straight, because\n",
    "it keeps bringing the same result.\n",
    "\"\"\"\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "\n",
    "scores = cross_val_score(xgboost, X_train, y_train, cv=cv, verbose=10)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.61      0.71      4565\n",
      "           1       0.56      0.06      0.10       863\n",
      "           2       0.71      0.94      0.81      6452\n",
      "\n",
      "    accuracy                           0.75     11880\n",
      "   macro avg       0.71      0.54      0.54     11880\n",
      "weighted avg       0.75      0.75      0.72     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.RandomForestClassifier(n_estimators=200, max_depth=10, min_samples_split=2)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators' : [10, 50, 200],\n",
    "          'min_samples_split' : [2, 3, 4],\n",
    "          'max_depth': [3, 5, 8, 10]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=clf, cv=3, param_grid=params, n_jobs=-1 verbose=10)\n",
    "\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SVM clearly not the best fit for this problem, because of the 100+ columns.\n",
    "\"\"\"\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc = svc.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = svc.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.61      0.68      4565\n",
      "           1       0.26      0.01      0.02       863\n",
      "           2       0.70      0.90      0.79      6452\n",
      "\n",
      "    accuracy                           0.72     11880\n",
      "   macro avg       0.58      0.51      0.50     11880\n",
      "weighted avg       0.70      0.72      0.69     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It doesn't perform well.\n",
    "\"\"\"\n",
    "\n",
    "logreg = LogisticRegression(multi_class='auto', solver='newton-cg', max_iter=50, n_jobs=-1)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = logreg.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GridSearch\n",
    "\"\"\"\n",
    "\n",
    "params = {'solver': ['newton-cg', 'sag', 'saga', 'lbfgs'],\n",
    "          'max_iter': [50, 150, 200]\n",
    "         }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=logreg, cv=5, param_grid=params, n_jobs=-1 verbose=10)\n",
    "\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       33369.2148           16.46m\n",
      "         2       31287.2605           17.02m\n",
      "         3       29545.9655           18.20m\n",
      "         4       28040.9708           17.90m\n",
      "         5       26702.7998           17.82m\n",
      "         6       25532.2280           17.96m\n",
      "         7       24497.6548           18.12m\n",
      "         8       23572.8610           17.89m\n",
      "         9       22752.6387           17.97m\n",
      "        10       21987.2140           17.67m\n",
      "        11       21311.4331           17.65m\n",
      "        12       20687.4989           17.48m\n",
      "        13       20126.3307           16.93m\n",
      "        14       19621.3449           16.46m\n",
      "        15       19155.8965           15.94m\n",
      "        16       18689.5576           15.58m\n",
      "        17       18282.6314           15.29m\n",
      "        18       17880.4935           15.00m\n",
      "        19       17548.9996           14.70m\n",
      "        20       17224.4807           14.70m\n",
      "        21       16928.6945           14.68m\n",
      "        22       16626.8833           14.68m\n",
      "        23       16380.7404           14.61m\n",
      "        24       16145.4878           14.59m\n",
      "        25       15898.7687           14.56m\n",
      "        26       15667.3149           14.52m\n",
      "        27       15450.8643           14.46m\n",
      "        28       15217.2979           14.39m\n",
      "        29       15026.9192           14.10m\n",
      "        30       14840.5859           13.76m\n",
      "        31       14660.0277           13.52m\n",
      "        32       14501.3463           13.16m\n",
      "        33       14340.6309           12.82m\n",
      "        34       14184.4123           12.50m\n",
      "        35       14030.8085           12.18m\n",
      "        36       13875.9756           11.87m\n",
      "        37       13764.8802           11.53m\n",
      "        38       13653.1708           11.25m\n",
      "        39       13535.3103           10.94m\n",
      "        40       13411.4736           10.64m\n",
      "        41       13291.4996           10.38m\n",
      "        42       13193.1335           10.07m\n",
      "        43       13093.3296            9.77m\n",
      "        44       12997.2217            9.49m\n",
      "        45       12896.5950            9.21m\n",
      "        46       12813.5807            8.94m\n",
      "        47       12732.8693            8.67m\n",
      "        48       12634.6259            8.41m\n",
      "        49       12536.6027            8.17m\n",
      "        50       12469.6903            7.91m\n",
      "        51       12416.9692            7.65m\n",
      "        52       12326.6847            7.42m\n",
      "        53       12279.2895            7.17m\n",
      "        54       12221.5559            6.94m\n",
      "        55       12179.8199            6.72m\n",
      "        56       12130.1607            6.49m\n",
      "        57       12059.6677            6.29m\n",
      "        58       12016.5056            6.07m\n",
      "        59       11945.3164            5.87m\n",
      "        60       11892.3917            5.67m\n",
      "        61       11842.7879            5.49m\n",
      "        62       11793.8006            5.30m\n",
      "        63       11735.9992            5.12m\n",
      "        64       11695.9182            4.93m\n",
      "        65       11649.4235            4.75m\n",
      "        66       11602.1438            4.57m\n",
      "        67       11530.7925            4.41m\n",
      "        68       11465.5396            4.24m\n",
      "        69       11412.7017            4.08m\n",
      "        70       11380.8310            3.91m\n",
      "        71       11308.8364            3.76m\n",
      "        72       11253.9824            3.61m\n",
      "        73       11178.1883            3.47m\n",
      "        74       11128.3148            3.32m\n",
      "        75       11055.1578            3.17m\n",
      "        76       10989.5717            3.04m\n",
      "        77       10951.6746            2.89m\n",
      "        78       10925.8696            2.74m\n",
      "        79       10889.9157            2.60m\n",
      "        80       10828.9317            2.46m\n",
      "        81       10782.8007            2.33m\n",
      "        82       10759.4830            2.19m\n",
      "        83       10725.6031            2.06m\n",
      "        84       10676.0514            1.92m\n",
      "        85       10638.4963            1.80m\n",
      "        86       10602.1319            1.67m\n",
      "        87       10554.5957            1.54m\n",
      "        88       10532.5390            1.41m\n",
      "        89       10502.5272            1.28m\n",
      "        90       10481.9594            1.16m\n",
      "        91       10442.0598            1.04m\n",
      "        92       10412.3356           55.10s\n",
      "        93       10378.8167           48.03s\n",
      "        94       10354.6886           40.97s\n",
      "        95       10323.1963           33.95s\n",
      "        96       10291.0179           27.00s\n",
      "        97       10261.2044           20.13s\n",
      "        98       10239.3023           13.33s\n",
      "        99       10197.9993            6.64s\n",
      "       100       10181.5953            0.00s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.80      4565\n",
      "           1       0.53      0.29      0.38       863\n",
      "           2       0.80      0.89      0.84      6452\n",
      "\n",
      "    accuracy                           0.80     11880\n",
      "   macro avg       0.72      0.65      0.67     11880\n",
      "weighted avg       0.79      0.80      0.79     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This was the best model, scored 80.11% on the DrivenData submission. It is really slow though.\n",
    "\"\"\"\n",
    "\n",
    "gradient = GradientBoostingClassifier(n_estimators=100, learning_rate=0.075, max_depth=14,\n",
    "                                     min_samples_leaf=16, verbose=5)\n",
    "\n",
    "gradient = gradient.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = gradient.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.76      0.79      2739\n",
      "           1       0.52      0.27      0.36       518\n",
      "           2       0.79      0.89      0.84      3871\n",
      "\n",
      "    accuracy                           0.79      7128\n",
      "   macro avg       0.71      0.64      0.66      7128\n",
      "weighted avg       0.79      0.79      0.79      7128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the validation set, which is unseen for the model.\n",
    "\"\"\"\n",
    "\n",
    "validation_expected = y_val\n",
    "validation_predicted = gradient.predict(X_val)\n",
    "\n",
    "print(metrics.classification_report(validation_expected, validation_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RandomizedSearchCV, to make it faster than GridSearchCV.\n",
    "\"\"\"\n",
    "gradient = GradientBoostingClassifier(verbose=5)\n",
    "\n",
    "params = {'n_estimators': [100, 150, 200, 250, 300],\n",
    "          'max_depth': [5, 8, 10, 15, 20],\n",
    "          'learning_rate': [0.075, 0.01, 0.05, 0.1],\n",
    "          'min_samples_leaf': [10, 15, 20, 25]\n",
    "         }\n",
    "\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(gradient, param_distributions=params,\n",
    "                                   n_iter=n_iter_search, n_jobs=-1, cv=3, verbose=11)\n",
    "\n",
    "random_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(random_search.best_score_)\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist Gradient Boosing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.75      0.79      4565\n",
      "           1       0.56      0.27      0.36       863\n",
      "           2       0.78      0.90      0.84      6452\n",
      "\n",
      "    accuracy                           0.79     11880\n",
      "   macro avg       0.73      0.64      0.66     11880\n",
      "weighted avg       0.79      0.79      0.78     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Histogram-based Gradient Boosting Classification Tree.\n",
    "\n",
    "This estimator is much faster than GradientBoostingClassifier for big datasets (n_samples >= 10 000).\n",
    "\"\"\"\n",
    "\n",
    "hist_gradient = HistGradientBoostingClassifier(learning_rate=0.2, loss='categorical_crossentropy', \n",
    "                                               max_depth=8, min_samples_leaf=15)\n",
    "\n",
    "hist_gradient = hist_gradient.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = hist_gradient.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.74      0.78      2739\n",
      "           1       0.57      0.25      0.35       518\n",
      "           2       0.77      0.90      0.83      3871\n",
      "\n",
      "    accuracy                           0.79      7128\n",
      "   macro avg       0.73      0.63      0.66      7128\n",
      "weighted avg       0.79      0.79      0.78      7128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the validation set, which is unseen for the model.\n",
    "\"\"\"\n",
    "\n",
    "validation_expected = y_val\n",
    "validation_predicted = hist_gradient.predict(X_val)\n",
    "\n",
    "print(metrics.classification_report(validation_expected, validation_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GridSearchCV, the most computation expensive part of the process.\n",
    "\"\"\"\n",
    "\n",
    "hist_gradient = HistGradientBoostingClassifier(verbose=2)\n",
    "\n",
    "params = {'max_iter': [100, 150, 200],\n",
    "          'loss': ['categorical_crossentropy'],\n",
    "          'max_depth': [5, 8, 15],\n",
    "          'learning_rate': [0.01, 0.75, 0.1, 0.2],\n",
    "          'min_samples_leaf': [5, 15, 20]\n",
    "         }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=hist_gradient, cv=4, param_grid=params, n_jobs=-1, verbose=10)\n",
    "\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.78      4565\n",
      "           1       0.61      0.24      0.34       863\n",
      "           2       0.77      0.91      0.84      6452\n",
      "\n",
      "    accuracy                           0.79     11880\n",
      "   macro avg       0.74      0.63      0.65     11880\n",
      "weighted avg       0.79      0.79      0.78     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LightGBM is another fast tree based gradient boosting algorithm, which supports GPU, and parallel learning.\n",
    "\"\"\"\n",
    "\n",
    "lgbm = LGBMClassifier(objective='multiclass', learning_rate=0.1, num_iterations=200, num_leaves=50, random_state=123)\n",
    "\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = lgbm.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.72      0.78      2739\n",
      "           1       0.61      0.23      0.34       518\n",
      "           2       0.77      0.91      0.83      3871\n",
      "\n",
      "    accuracy                           0.79      7128\n",
      "   macro avg       0.74      0.62      0.65      7128\n",
      "weighted avg       0.79      0.79      0.78      7128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing the validation set, which is unseen for the model.\n",
    "\"\"\"\n",
    "\n",
    "validation_expected = y_val\n",
    "validation_predicted = lgbm.predict(X_val)\n",
    "\n",
    "print(metrics.classification_report(validation_expected, validation_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GridSearchCV, the most computation expensive part of the process.\n",
    "\"\"\"\n",
    "\n",
    "lgbm = LGBMClassifier(objective='multiclass', num_threads=2, verbose=2, random_state=123)\n",
    "\n",
    "params = {'num_iterations ': [100, 150, 200],\n",
    "          'max_depth': [5, 8, 15],\n",
    "          'learning_rate': [0.01, 0.75, 0.1, 0.2],\n",
    "          'num_leaves' : [25, 40, 50]\n",
    "         }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=lgbm, cv=4, param_grid=params, n_jobs=-1, verbose=5) # n_jobs=-1 = use all the CPU cores\n",
    "\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras wrapper for Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86752577, 4.58583106, 0.61378556])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "For imbalanced datasets like this.\n",
    "\"\"\"\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40392 samples\n",
      "Epoch 1/30\n",
      "40392/40392 [==============================] - 20s 485us/sample - loss: 2.2383 - accuracy: 0.5833\n",
      "Epoch 2/30\n",
      "40392/40392 [==============================] - 19s 474us/sample - loss: 0.9307 - accuracy: 0.6611\n",
      "Epoch 3/30\n",
      "40392/40392 [==============================] - 19s 460us/sample - loss: 0.8250 - accuracy: 0.6753\n",
      "Epoch 4/30\n",
      "40392/40392 [==============================] - 17s 417us/sample - loss: 0.7509 - accuracy: 0.6836\n",
      "Epoch 5/30\n",
      "40392/40392 [==============================] - 16s 408us/sample - loss: 0.7546 - accuracy: 0.6926\n",
      "Epoch 6/30\n",
      "40392/40392 [==============================] - 14s 350us/sample - loss: 0.7253 - accuracy: 0.6984\n",
      "Epoch 7/30\n",
      "40392/40392 [==============================] - 13s 318us/sample - loss: 0.7321 - accuracy: 0.7004\n",
      "Epoch 8/30\n",
      "40392/40392 [==============================] - 15s 361us/sample - loss: 0.7157 - accuracy: 0.7054\n",
      "Epoch 9/30\n",
      "40392/40392 [==============================] - 14s 344us/sample - loss: 0.7131 - accuracy: 0.7055\n",
      "Epoch 10/30\n",
      "40392/40392 [==============================] - 14s 338us/sample - loss: 0.7134 - accuracy: 0.7071\n",
      "Epoch 11/30\n",
      "40392/40392 [==============================] - 14s 354us/sample - loss: 0.7218 - accuracy: 0.7087\n",
      "Epoch 12/30\n",
      "40392/40392 [==============================] - 13s 323us/sample - loss: 0.7049 - accuracy: 0.7097\n",
      "Epoch 13/30\n",
      "40392/40392 [==============================] - 14s 346us/sample - loss: 0.7069 - accuracy: 0.7125\n",
      "Epoch 14/30\n",
      "40392/40392 [==============================] - 16s 396us/sample - loss: 0.7137 - accuracy: 0.7112\n",
      "Epoch 15/30\n",
      "40392/40392 [==============================] - 16s 397us/sample - loss: 0.6993 - accuracy: 0.7132\n",
      "Epoch 16/30\n",
      "40392/40392 [==============================] - 16s 386us/sample - loss: 0.6952 - accuracy: 0.7150\n",
      "Epoch 17/30\n",
      "40392/40392 [==============================] - 17s 416us/sample - loss: 0.6972 - accuracy: 0.7137\n",
      "Epoch 18/30\n",
      "40392/40392 [==============================] - 17s 425us/sample - loss: 0.6916 - accuracy: 0.7165\n",
      "Epoch 19/30\n",
      "40392/40392 [==============================] - 14s 353us/sample - loss: 0.6927 - accuracy: 0.7165\n",
      "Epoch 20/30\n",
      "40392/40392 [==============================] - 17s 418us/sample - loss: 0.6937 - accuracy: 0.7166\n",
      "Epoch 21/30\n",
      "40392/40392 [==============================] - 17s 424us/sample - loss: 0.6915 - accuracy: 0.7166\n",
      "Epoch 22/30\n",
      "40392/40392 [==============================] - 16s 385us/sample - loss: 0.6901 - accuracy: 0.7186\n",
      "Epoch 23/30\n",
      "40392/40392 [==============================] - 21s 525us/sample - loss: 0.6870 - accuracy: 0.7209\n",
      "Epoch 24/30\n",
      "40392/40392 [==============================] - 19s 478us/sample - loss: 0.6886 - accuracy: 0.7204\n",
      "Epoch 25/30\n",
      "40392/40392 [==============================] - 17s 421us/sample - loss: 0.6971 - accuracy: 0.7215\n",
      "Epoch 26/30\n",
      "40392/40392 [==============================] - 17s 411us/sample - loss: 0.6838 - accuracy: 0.7204\n",
      "Epoch 27/30\n",
      "40392/40392 [==============================] - 21s 523us/sample - loss: 0.6852 - accuracy: 0.7229\n",
      "Epoch 28/30\n",
      "40392/40392 [==============================] - 16s 386us/sample - loss: 0.6831 - accuracy: 0.7237\n",
      "Epoch 29/30\n",
      "40392/40392 [==============================] - 20s 489us/sample - loss: 0.6817 - accuracy: 0.7229\n",
      "Epoch 30/30\n",
      "40392/40392 [==============================] - 20s 486us/sample - loss: 0.6812 - accuracy: 0.7249\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "11880/11880 [==============================] - 3s 247us/sample\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.62      0.68      4565\n",
      "           1       0.56      0.04      0.07       863\n",
      "           2       0.71      0.88      0.78      6452\n",
      "\n",
      "    accuracy                           0.72     11880\n",
      "   macro avg       0.67      0.51      0.51     11880\n",
      "weighted avg       0.71      0.72      0.69     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "0    15520\n",
    "1     2936\n",
    "2    21936\n",
    "\n",
    "Manual class weight wasn't as good, as 'balanced'.\n",
    "\"It basically means replicating the smaller class until you have as many samples as in the larger one, but in an implicit way.\"\n",
    "\n",
    "It needs more tuning / layers, because it stucks at 71% + set an early stopping.\n",
    "\"\"\"\n",
    "\n",
    "def model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model, epochs=30, batch_size=5, verbose=1)\n",
    "estimator.fit(X_train.values, y_train.values, class_weight=class_weights)\n",
    "\n",
    "expected = y_test\n",
    "predicted = estimator.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPClassifier - Neural network for scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.37      0.37      4565\n",
      "           1       0.08      0.06      0.07       863\n",
      "           2       0.53      0.56      0.55      6452\n",
      "\n",
      "    accuracy                           0.45     11880\n",
      "   macro avg       0.33      0.33      0.33     11880\n",
      "weighted avg       0.44      0.45      0.44     11880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It needs tuning.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=0.0001,\n",
    "                     hidden_layer_sizes=(50, 50, 50, 50), max_iter=500, random_state=1, verbose=10)\n",
    "mlp.out_activation_ = 'softmax'\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "expected = y_test\n",
    "predicted = mlp.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       33368.7480           14.45m\n",
      "         2       31288.4000           14.12m\n",
      "         3       29545.8449           13.30m\n",
      "         4       28040.6424           12.77m\n",
      "         5       26706.5589           12.75m\n",
      "         6       25547.2156           12.50m\n",
      "         7       24516.4368           12.18m\n",
      "         8       23578.1669           11.97m\n",
      "         9       22756.1825           11.72m\n",
      "        10       21989.8351           11.58m\n",
      "        11       21302.6141           11.42m\n",
      "        12       20675.5112           11.39m\n",
      "        13       20112.2442           11.35m\n",
      "        14       19599.8916           11.24m\n",
      "        15       19122.8282           11.21m\n",
      "        16       18698.1707           11.05m\n",
      "        17       18287.5867           10.82m\n",
      "        18       17900.1732           10.61m\n",
      "        19       17544.2573           10.40m\n",
      "        20       17234.3065           10.25m\n",
      "        21       16909.7891           10.19m\n",
      "        22       16637.6857           10.03m\n",
      "        23       16364.7569            9.82m\n",
      "        24       16142.0069            9.68m\n",
      "        25       15895.3064            9.49m\n",
      "        26       15656.0962            9.31m\n",
      "        27       15442.1761            9.13m\n",
      "        28       15241.7750            8.95m\n",
      "        29       15036.5023            8.79m\n",
      "        30       14851.7243            8.69m\n",
      "        31       14666.0694            8.64m\n",
      "        32       14519.9718            8.49m\n",
      "        33       14355.4732            8.44m\n",
      "        34       14198.8266            8.33m\n",
      "        35       14063.4562            8.19m\n",
      "        36       13912.0890            8.07m\n",
      "        37       13768.1649            7.93m\n",
      "        38       13647.0752            7.80m\n",
      "        39       13520.7946            7.64m\n",
      "        40       13419.2150            7.50m\n",
      "        41       13313.1083            7.35m\n",
      "        42       13222.3768            7.17m\n",
      "        43       13130.6139            6.99m\n",
      "        44       13006.3371            6.83m\n",
      "        45       12892.8469            6.67m\n",
      "        46       12806.1970            6.50m\n",
      "        47       12714.5892            6.33m\n",
      "        48       12636.9367            6.19m\n",
      "        49       12558.3476            6.08m\n",
      "        50       12499.7061            5.92m\n",
      "        51       12407.9181            5.79m\n",
      "        52       12346.8832            5.64m\n",
      "        53       12245.0889            5.50m\n",
      "        54       12192.5802            5.33m\n",
      "        55       12132.8334            5.19m\n",
      "        56       12049.7784            5.05m\n",
      "        57       11973.4039            4.91m\n",
      "        58       11912.7240            4.75m\n",
      "        59       11864.6723            4.61m\n",
      "        60       11825.9931            4.47m\n",
      "        61       11777.3461            4.34m\n",
      "        62       11731.1354            4.19m\n",
      "        63       11667.9255            4.07m\n",
      "        64       11618.8220            3.93m\n",
      "        65       11572.8652            3.79m\n",
      "        66       11517.5252            3.66m\n",
      "        67       11476.5205            3.52m\n",
      "        68       11417.7597            3.41m\n",
      "        69       11369.2377            3.28m\n",
      "        70       11340.0228            3.15m\n",
      "        71       11293.8589            3.02m\n",
      "        72       11267.4807            2.89m\n",
      "        73       11218.3785            2.77m\n",
      "        74       11168.8622            2.65m\n",
      "        75       11111.4026            2.54m\n",
      "        76       11079.1294            2.43m\n",
      "        77       11047.1581            2.32m\n",
      "        78       10992.6653            2.21m\n",
      "        79       10949.0839            2.10m\n",
      "        80       10885.0627            1.99m\n",
      "        81       10844.6664            1.88m\n",
      "        82       10795.7189            1.78m\n",
      "        83       10729.3304            1.68m\n",
      "        84       10675.8214            1.58m\n",
      "        85       10632.7560            1.47m\n",
      "        86       10594.7292            1.37m\n",
      "        87       10551.9284            1.27m\n",
      "        88       10507.1864            1.17m\n",
      "        89       10454.0239            1.07m\n",
      "        90       10421.9148           57.85s\n",
      "        91       10381.4781           51.90s\n",
      "        92       10331.7374           45.99s\n",
      "        93       10283.1388           40.15s\n",
      "        94       10229.6659           34.30s\n",
      "        95       10202.1828           28.46s\n",
      "        96       10162.7026           22.70s\n",
      "        97       10126.5604           16.95s\n",
      "        98       10082.7785           11.26s\n",
      "        99       10058.5937            5.60s\n",
      "       100       10029.7538            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7984848484848485"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You can combine your best predictors as a VotingClassifier, which can enhance the performance.\n",
    "\n",
    "26/01/2020 - The hist_gradient / lgbm / gradient combination made the best result yet: 80.35%!\n",
    "\"\"\"\n",
    "\n",
    "estimators = [('hist_gradient', hist_gradient), ('lightgbm', lgbm), ('gradient', gradient)]\n",
    "\n",
    "ensemble = VotingClassifier(estimators, voting='hard', verbose=10)\n",
    "\n",
    "ensemble.fit(X_train, y_train)\n",
    "ensemble.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14850 entries, 0 to 14849\n",
      "Data columns (total 2 columns):\n",
      "id              14850 non-null int64\n",
      "status_group    14850 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 232.2+ KB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The 'status_group_nums' should be changed with the model we want to predict.\n",
    "\"\"\"\n",
    "\n",
    "submission = pd.read_csv('https://raw.githubusercontent.com/budapestpy-workshops/workshops/master/09_pump_it_up/SubmissionFormat.csv')\n",
    "submission['status_group_nums'] = ensemble.predict(test_selected)\n",
    "\n",
    "label_dict = {2: \"functional\", 1: \"functional needs repair\", 0 :\"non functional\"}\n",
    "submission[\"status_group\"] = submission[\"status_group_nums\"].map(label_dict)\n",
    "submission = submission.drop('status_group_nums', axis=1)\n",
    "\n",
    "submission.to_csv('pumpitup.csv',index=False)\n",
    "\n",
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DataCamp: Feature engineering with Python course:\n",
    "\n",
    "One-hot and dummy encoding, drop_first=True, because if not specified the model could be distracted by duplicate info.\n",
    "Numerical columns: binning with pd.cut, so if we have ten different values in a column, but three of them are the 70% of all,\n",
    "the other 7 could be \"other\".\n",
    "\n",
    "Scaling: StandardScaler does not affect the data's shape. This works great if your data is normally distributed \n",
    "(or closely normally distributed), an assumption that a lot of machine learning models make. Sometimes you will work \n",
    "with data that closely conforms to normality, e.g the height or weight of a population. On the other hand, many variables \n",
    "in the real world do not follow this pattern e.g, wages or age of a population. In this exercise you will use a log \n",
    "transform on the ConvertedSalary column as it has a large amount of its data centered around the lower values, \n",
    "but contains very high values also. These distributions are said to have a long right tail.\n",
    "We should try the PowerTransformer for these kind of columns.\n",
    "\n",
    "Though Tree based models won't affect by the standardization.\n",
    "\n",
    "Fit and transform on the TRAINING data, and ONLY TRANSFORM on the TEST data. This is to avoid data leakage, because in real life\n",
    "we won't have access to test data, so we have to work with what we have, when making the model. Calibrate the preprocessing\n",
    "steps only on the training data. To do this in practice you train the scaler on the train set, and keep the trained scaler \n",
    "to apply it to the test set. You should never retrain a scaler on the test set. This approach is for scaling, and for removing\n",
    "outilers too.\n",
    "\n",
    "I should check the distributions of the columns to look fo outliers. Eliminate data with 3 std\n",
    "(this is the more statistical approach), or the 95th quantile (column.quantile(0.95)).\n",
    "\n",
    "More:\n",
    "\n",
    "- VotingClassifier check. https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a\n",
    "    74.5% but the Logistic regression, the third model may not be the best choice.\n",
    "\n",
    "- 'construction_year' binning. Check out the earliest, and the latest dates. - This made from 75.20% to 75.25%. \n",
    "   Very small step, but helped.\n",
    "   \n",
    "- Dimension reduction - needs scaling first. Try with just scaling, and then with scaling and PCA. - \n",
    "  It didn't helped for first try.\n",
    "  \n",
    "- 'functional needs repair' oversampling - DrivenData 70.46%.\n",
    "\n",
    "- DL solution + LightGBM\n",
    "\n",
    "- log transformation - done, 74.55% was the driven data accuracy, on my validation set, it was 75% so it was 1% better. \n",
    "\n",
    "- perc_func_lga - A column I created in a past version of this notebook.\n",
    "\n",
    "- Delete validation set, because it means 7000 rows.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
